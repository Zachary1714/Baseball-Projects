---
title: "Lahman Career WAR Predictions"
author: "Zach Houghtaling"
date: "2023-08-25"
output: html_document
---

```{r setup, include=FALSE, knitr root.dir = "~/GitHub/baseball-projects/Baseball-Projects/Lahman-Career-WAR-Predictions" }
knitr::opts_chunk$set(echo = TRUE)

```


```{r Load in Libraries}
library(tidyverse)      
library(readxl)
library(glmnet)   
library(randomForest)
library(xgboost)
library(car)
library(lmtest)
library(MASS)
library(nortest)
library(DescTools) 
library(e1071)
library(caret)
library(tictoc)
library(rstan)
library(corrplot)
library(Ckmeans.1d.dp)
library(pdp)
library(readxl)
library(iml)
library(SHAPforxgboost)
library(keras)
```


Read in the files from your working directory

```{r Load in Data}
org_batting <- read.csv("Batting.csv")      #Table containing hitting statistics
org_people <- read.csv("People.csv")          #Table containing birthdays
org_appearance <- read.csv("Appearances.csv") #Table containing games played in
org_ranking <- read_xlsx("historic_prospect_ranking_with_Lahman_ID.xlsx")

```


```{r Initial Data Manipulation}
#Filter for years 1994 and beyond but still hold the original tables for reference
batting <- org_batting %>% filter(yearID >= 1994)
appearance <- org_appearance %>% filter(yearID >= 1994)
people <- org_people %>% filter(finalGame >= 1994)

#Rename some columns in the ranking table and remove players who did not make the big leagues
ranking <- org_ranking %>% rename(playerID=lahman_id)
ranking <- na.omit(ranking) #Remove ranked players that never made the big leagues
```




```{r}
#Acquire a player's best ranking ever earned. This will be our basis for their minor league performance evaluation
best_rank <- ranking %>% 
  group_by(playerID) %>% 
  summarise(best_ranking=min(prospect_rank)) %>%
  left_join(ranking,by="playerID") %>% 
  filter(best_ranking==prospect_rank) %>%
  rename(best_ranked_year=year) %>%
  arrange(playerID,desc(best_ranked_year)) %>%
  subset(!duplicated(playerID))

#Adding plate appearances, average, singles, slugging, on-base %, and OPS
batting <- batting %>% 
  mutate(PA=(AB+BB+HBP+SF+SH+IBB),                      #PA Calculation
         AVG=(H/AB),                                    #AVG Calculation
         X1B=(H-(X2B+X3B+HR)),                          #Singles Calculation
         SB_perc=ifelse((SB+CS)==0,0,SB/(SB+CS)),       #Stolen Base Percentage Calculation (This will be 0 if never attempted) 
         eligible=T,                                    #Indicator of whether they are age 31 (Will be changed later based on each row)
         ID=paste(playerID,yearID,teamID,sep = " ")) %>%#ID to merge data frames         
  filter(PA != 0) %>%                                   #Removing all occurrences of no plate appearances
  mutate(SLG=((X1B+(2*X2B)+(3*X3B)+(4*HR))/AB ),        #Slugging Percentage Calculation
         OBP=((H+BB+HBP+IBB)/PA),                       #On-Base Percentage Calculation
         BABIP=ifelse((AB-HR-SO+SF)==0,0,((H-HR)/(AB-HR-SO+SF)))) %>%          #Batting Average on Balls in Play Calc
  mutate(AVG=ifelse(AVG=="NaN",0,AVG),                                         #Change NaN to 0 since they had no at-bats
         SLG=ifelse(SLG=="NaN",0,SLG),
         OBP=ifelse(OBP=="NaN",0,OBP)) %>%
  mutate(OPS=(SLG+OBP)) %>%                                                    #OPS Calculation
  left_join(best_rank, by = "playerID")                                        #Merge the prospect rankings with the batting table

```

```{r}
###CALCULATING LEAGUE AVERAGE OPS BASED ON AMERICAN LEAGUE###
league_average_ops <- batting %>%
  na.omit() %>%                  #Remove the NA cases
  filter(lgID=="AL") %>%         #Only use American League per request
  dplyr::select(yearID,OPS) %>%  #Take the two columns we care about
  group_by(yearID) %>%           #Group the years together
  summarize_all(mean) %>%        #Get the average
  rename(avg_OPS=OPS)

#TIE THE LEAGUE AVERAGE OPS BACK IN TO GET THE OFFENSIVE RAA VALUE###
batting <- batting %>%
  left_join(league_average_ops, by = "yearID") %>%
  mutate(o_raa_total=(PA * (OPS - avg_OPS) /3.2135))

```

```{r}
#Loop to bin the rankings into 16 different categories of unranked, 1, 2, 3, 4, 5, 6-10, then by 10's from 11-20 to 91-100
for(i in 1:nrow(batting)){
  if (is.na(batting$best_ranking[i])){        #If their best ranking is NA, 
    batting$best_ranking[i] <- "Never Ranked" #they were never ranked
  }else if (batting$best_ranking[i] == 1){    #If it is 1, 
    batting$best_ranking[i] <- "1"            #their best ranking was "1"
  }else if (batting$best_ranking[i] == 2){    #and so on.
    batting$best_ranking[i] <- "2"
  }else if (batting$best_ranking[i] == 3){
    batting$best_ranking[i] <- "3"
  }else if (batting$best_ranking[i] == 4){
    batting$best_ranking[i] <- "4"
  }else if (batting$best_ranking[i] == 5){
    batting$best_ranking[i] <- "5"
  }else if (batting$best_ranking[i] <= 10){  #To condense bins, 6-10 is matched up
    batting$best_ranking[i] <- "6-10"
  }else if (batting$best_ranking[i] <= 20){  #The rest will be done by 10's
    batting$best_ranking[i] <- "11-20"
  }else if (batting$best_ranking[i] <= 30){
    batting$best_ranking[i] <- "21-30"
  }else if (batting$best_ranking[i] <= 40){
    batting$best_ranking[i] <- "31-40"
  }else if (batting$best_ranking[i] <= 50){
    batting$best_ranking[i] <- "41-50"
  }else if (batting$best_ranking[i] <= 60){
    batting$best_ranking[i] <- "51-60"
  }else if (batting$best_ranking[i] <= 70){
    batting$best_ranking[i] <- "61-70"
  }else if (batting$best_ranking[i] <= 80){
    batting$best_ranking[i] <- "71-80"
  }else if (batting$best_ranking[i] <= 90){
    batting$best_ranking[i] <- "81-90"
  }else {
    batting$best_ranking[i] <- "91-100"
  }
}

```

```{r}
##GET ALL THE POSITION PLAYERS (Accounting for some pitching appearances in blowouts/two-way players)###
appearance <- appearance %>% 
  mutate(G_field = (G_c+G_1b+G_2b+G_3b+G_ss+G_of+G_dh)) %>% #All non-pitching games played
  filter(G_p == 0 | G_field >= 10) %>%                      #A player who hasn't pitched or appears in 10 or more games in the field is included
  filter(G_field != 0) %>%                                  #Remove players who have only been pinch hitters or pinch runners
  mutate(raa_c=(G_c/G_field)*(9/150)*GS,                    #Calculate the defensive RAA for each position
         raa_1b=(G_1b/G_field)*(-9.5/150)*GS,               #G_field is used instead of G_defense to include the DH while still removing games appeared as a pitcher, pinch hitter, or pinch runner 
         raa_2b=(G_2b/G_field)*(3/150)*GS,
         raa_3b=(G_3b/G_field)*(2/150)*GS,
         raa_ss=(G_ss/G_field)*(7/150)*GS,
         raa_lf=(G_lf/G_field)*(-7/150)*GS,
         raa_cf=(G_cf/G_field)*(2.5/150)*GS,
         raa_rf=(G_rf/G_field)*(-7/150)*GS,
         raa_dh=(G_dh/G_field)*(-15/150)*GS) %>%
  mutate(d_raa_total=(raa_c+raa_1b+raa_2b+raa_3b+raa_ss+raa_lf+raa_cf+raa_rf+raa_dh)) %>%
  mutate(ID=paste(playerID,yearID,teamID,sep = " ")) %>%          #ID to match the players to the batting table
  dplyr::select(-c(yearID,teamID,lgID,playerID))           #Remove columns that will overlap with the batting table when joined

```

```{r Add in birthdates}
###BRING IN BIRTHDATES TO SEE WHO IS OVER 31###
batting <- batting %>% 
  left_join(people, by="playerID") %>%
  mutate(age=ifelse(birthMonth>=7,yearID-birthYear-1,yearID-birthYear)) %>% #Get their age for the season
  mutate(eligible=ifelse(age < 31, T, F)) #If they are 30 or less for the season, they are included
   
```

```{r Combine the appearances and batting tables}
total <- batting %>%
  right_join(appearance,by="ID") %>%           #For each person's 
  filter(eligible==T) %>%
  mutate(RAR=(o_raa_total+d_raa_total+20)) %>%
  mutate(WAR=ifelse(RAR<0,0,RAR/10),
         ID=paste(ID,stint)) %>%
  unique()
```

```{r Remove players without WAR}
###REMOVE CASES WITHOUT A WAR VALUE THEN SUM THEM###
total <- total[complete.cases(total$WAR),]
WAR_total <- total %>%
  group_by(playerID) %>%
  dplyr::select(playerID,WAR) %>%
  summarize_all(sum) %>%
  rename(WAR_pre31=WAR) #Get the sum of the season WAR values
#Combine the tables
total <- total %>%
  left_join(WAR_total,by="playerID")
```

```{r Player WAR remaining before 30 Calculation}
#Loop to calculate the players remaining WAR until age 30 inclusive of the current season
total <- total %>% arrange(playerID,yearID)      #Sort by each player
total$years_left <- 0                            #Set their remaining years in the league after the current season pre-31 to 0
total$remainingWAR <- 0                          #Set their remaining pre-31 WAR to 0
for (i in 1:(nrow(total)-5)){                    #For each case (through the penultimate playerID to avoid an error)
  k=i                                            #placeholder of index
  while(total$playerID[i]==total$playerID[k+1]){ #While we are still on the same player... 
    total$years_left[i] <- (k+1)-i                   #Their years left is however many rows that player has
    k <- k + 1
  }
  j=1
  for(j in 0:total$years_left[i]){
    total$remainingWAR[i] <- total$remainingWAR[i]+total$WAR[i+j]
  }
}
#The last player needs to be hard coded since there is no row after him
for (i in (nrow(total)-5):nrow(total)){
  total$years_left[i] <- nrow(total)-i
  j=1
  for(j in 0:total$years_left[i]){
    total$remainingWAR[i] <- total$remainingWAR[i]+total$WAR[i+j]
  }
}

total$previous_accurred_WAR <- ifelse(total$WAR_pre31-total$remainingWAR < 0.0000001,0,total$WAR_pre31-total$remainingWAR)
total$remainingWAR <- total$remainingWAR-total$WAR

```

```{r}
###REMOVE VARIABLES THAT ARE REPETITIVE, HAVE TOO MANY LEVELS, OR ARE UNUSEFUL###
#The reason why years left is not useful is we don't know how many years they have left for a current player
total <- total %>%
  dplyr::select(-c(eligible,birthCity,deathCountry,deathState,deathCity,deathYear,deathMonth,deathDay,nameFirst,nameLast,nameGiven,retroID,bbrefID,prospect_rank,team_abbr,first_name,last_name,mlbam_id,position,avg_OPS,o_raa_total,years_left)) %>%
  mutate(debut=as.numeric(as.Date(debut)-8766),         #Convert dates to days since 1994
         finalGame=as.numeric(as.Date(finalGame))-8766,
         PA_per_GS=(PA/G_field)*GS,                     #Additional rate statistics based on PA & Games played
         HR_per_PA=ifelse(PA==0,0,HR/PA),
         X2B_per_PA=ifelse(PA==0,0,X2B/PA),
         X3B_per_PA=ifelse(PA==0,0,X3B/PA),
         RBI_per_PA=ifelse(PA==0,0,RBI/PA),
         SB_per_H=ifelse(H==0,0,SB/H),
         CS_per_H=ifelse(H==0,0,CS/H),
         SO_per_PA=ifelse(PA==0,100,SO/PA),
         BB_per_PA=ifelse(PA==0,0,BB/PA))

#(Optional) Add variables if the player is still likely active or if they could have more unaccounted for WAR 
total <- total %>% mutate(More_WAR_after_dataset=ifelse(finalGame>(as.numeric(as.Date("2017-08-01"))-8766) & age < 30, T, F),
                          Previous_WAR_before_dataset=ifelse(debut<0,T,F))

```

```{r}
#Find how long after their best prospect ranking it took for the player to debut
for(i in 1:nrow(total)){
  if(is.na(total$best_ranked_year[i])){
    total$best_ranked_year[i] <- 10000000  #A big number is used to differentiate between unranked and ranked players
  }
  else{
    total$best_ranked_year[i] <- as.numeric(as.Date(paste0(as.character(total$best_ranked_year[i]),"-01-01")))-8766
  }
}
total$best_rank_debut_diff <- total$debut-total$best_ranked_year

```

```{r}
#Loop to bin how long prior to a player's debut their best ranking was
for(i in 1:nrow(total)){
  if(total$best_rank_debut_diff[i] < -100000){
    total$years_before_debut_best_rank[i] <- "Never Ranked"
  } else if(total$best_rank_debut_diff[i] < 0){
    total$years_before_debut_best_rank[i] <- "Highest Rank occurred after debut"
  } else if(total$best_rank_debut_diff[i] < 365){
    total$years_before_debut_best_rank[i] <- "Highest Rank 1 year before debut"
  } else if(total$best_rank_debut_diff[i] < 730){
    total$years_before_debut_best_rank[i] <- "Highest Rank 2 years before debut"
  } else if(total$best_rank_debut_diff[i] < 1095){
    total$years_before_debut_best_rank[i] <- "Highest Rank 3 years before debut"
  } else {
    total$years_before_debut_best_rank[i] <- "Highest Rank 4 or more years before debut"
  }
}
#Convert it to a categorical variable
total$years_before_debut_best_rank <- factor(total$years_before_debut_best_rank)

```

```{r}
#Calculate the player's primary position that season
catch_col <- which(colnames(total)=="G_c") #Get the column number for Games played at catcher
total$primary_pos <- factor(max.col(total[,c(catch_col:(catch_col+7),catch_col+9)],ties.method = "first")+1) #Check the maximum Games Played of the next 9 columns except for G_of

#Remove some columns that could be considered redundant and would cause multicollinearity
#This line must be run as these variables are too indicative of a players WAR and the year/date should not be a factor for a generalizable model
total <- total %>% dplyr::select(-c(yearID,birthYear,raa_c,raa_1b,raa_2b,raa_3b,raa_ss,raa_lf,raa_cf,raa_rf,RAR,debut,finalGame,raa_dh,d_raa_total,best_ranked_year,best_rank_debut_diff,WAR_pre31,playerID))
#This can be technically commented out if doing non-parametric tests
total <- total %>% dplyr::select(-c(PA,H,AB,birthState,OPS,X1B,G_p,G_c,G_1b,G_2b,G_3b,G_ss,G_lf,G_cf,G_rf,G_all,GS,G_field,G_dh,G_of,G_batting,G_defense,G_ph,G_pr,)) #PA, AB, and all the Game variables need to be removed since it is too highly correlated with PA/GS


#Option to convert string to factor if not done prior (Should not be done with ID still in total)
###CONVERT ALL REMAINING STRING VARIABLES TO CATEGORICAL###
#total <- as.data.frame(unclass(total),          
#                       stringsAsFactors = TRUE)
```


```{r}
#Remove players age 30 seasons as there is nothing to predict for those cases
#This is done now and not earlier as we need the age 30 season for their WAR before age 31
predictors <- total %>% 
  filter(age < 30) %>%
  filter(More_WAR_after_dataset == F) %>%
  dplyr::select(-c(More_WAR_after_dataset)) %>%
  mutate(birthCountry=ifelse(birthCountry=="P.R."|birthCountry=="Guam"|birthCountry=="V.I.","US_Territory",birthCountry)) %>%
  mutate(birthCountry=ifelse(birthCountry=="Nicaragua"|birthCountry=="Honduras"|birthCountry=="Panama","Central American Country",birthCountry)) %>%
  mutate(birthCountry=ifelse(birthCountry=="Australia"|birthCountry=="South Africa"|birthCountry=="Belgium"|birthCountry=="Saudi Arabia","Other",birthCountry)) %>%
  dplyr::select(remainingWAR, everything())
```

```{r Modeling Setup}
###SPLIT THE DATASET FOR MODELING###
set.seed(17) #Set a seed so the split can be replicated
predictors <- predictors %>% mutate(id = row_number())  #Split it into 60% for training, 30% for validation, and 10% for testing (testing is never used in this analysis)
train <- predictors %>% sample_frac(0.6)
validation <- anti_join(predictors, train, by = 'id') 
set.seed(17)
test <- validation %>% sample_frac(0.25)
validation <- anti_join(validation, test, by = 'id')

#Hold the players full ID to check abnormal observations in the end
id_num <- which(colnames(train)=="ID")
train_id <- train[id_num]
validation_id <- validation[id_num]
test_id <- test[id_num]

#Remove the id variable which is just the row number from each data frame
train <- train %>% dplyr::select(-c(id,ID))          
validation <- validation %>% dplyr::select(-c(id,ID))
test <- test %>% dplyr::select(-c(id,ID))

#Convert all the independent variables into a matrix to turn the categorical variables into dummy variables
train_x <- model.matrix(remainingWAR ~ ., data = train)[, -1] 
train_y <- train$remainingWAR
validation_x <- model.matrix(remainingWAR ~ ., data = validation)[, -1]
validation_y <- validation$remainingWAR
test_x <- model.matrix(remainingWAR ~ ., data = test)[, -1]
test_y <- test$remainingWAR
```

First we want to try and create an interpretable model. This 

```{r Testing Multicollinearyity}
#TESTING FOR MULTICOLLINEARITY#

#Remove factors to calculate the correlation matrix
factors <- c(  which(colnames(train)=="teamID"),
               which(colnames(train)=="lgID"),
               which(colnames(train)=="best_ranking"),
               which(colnames(train)=="birthCountry"),
               which(colnames(train)=="bats"),
               which(colnames(train)=="throws"),
               which(colnames(train)=="years_before_debut_best_rank"),
               which(colnames(train)=="primary_pos")
            )
correlation <- cor(train[,-c(factors)])
corrplot(correlation, type="upper", order="hclust", tl.col="black",tl.cex = 0.5)
train_uncorr_variables <- train %>%
  dplyr::select(-c(AVG,X2B,BB,SO,RBI,HR,R))
validation_uncorr_variables <- validation %>%
  dplyr::select(-c(AVG,X2B,BB,SO,RBI,HR,R))
test_uncorr_variables <- test %>%
  dplyr::select(-c(AVG,X2B,BB,SO,RBI,HR,R))

#Can check the vifs also to evaluate multicollinearity (Will not work if row 258 is commented out)
vif(lm(remainingWAR~.-best_ranking,data=train_uncorr_variables)) #High VIF for BABIP,each type of hit, and PA

```

```{r}
#Residual plot analysis
#Loop to test how well each included variable predicts remaining WAR with the residual plot
###-------
#remainWAR_col <- which(colnames(train)=="remainingWAR")
#for (i in c(1:(remainWAR_col-1),(remainWAR_col+1):length(train))){
#  lm <- lm(train$remainingWAR~unlist(train[[i]]))
#  print(summary(lm))
#  print(ggplot(lm,aes(x=train$remainingWAR,y=rstandard(lm))) +
#    geom_point(color="blue") +
#    geom_line(y=-3)+
#    geom_line(y=3)+
#    ylim(-10,10)+
#    labs(x="Remaining WAR",y="Residuals")+
#    ggtitle(paste0("Residual Plot of ",colnames(train[i]))))
#}
###---------

```

```{r}
#TESTING FOR NORMALITY#
n.index <- seq(1,nrow(train))
lm <- lm(remainingWAR~.,data=train)
ggplot(lm,aes(x=n.index,y=rstudent(lm))) +
  geom_point(color="blue") +
  geom_line(y=-3)+
  geom_line(y=3)+
  ylim(-10,10)+
  labs(x="Observation",y="Residuals")+
  ggtitle("Studentized Residuals")

ggplot(lm,aes(x=train$remainingWAR,y=rstandard(lm))) +
  geom_point(color="blue") +
  geom_line(y=-3)+
  geom_line(y=3)+
  ylim(-10,10)+
  labs(x="Remaining WAR",y="Residuals")+
  ggtitle("Residual Plot")

```


```{r}
#Tests for transformations of the data
#boxCox(lm) 
#yjPower(lm)
#Neither work as there is 0 as a potential outcome for WAR remaining

```


```{r}
hist(resid(lm),xlab="Residual Value",main="Histogram of Residuals")
qqnorm(resid(lm))
ad.test(resid(lm))  #Test if they are normal based on empirical distribution

```


```{r}
#Show which observation numbers have abnormally high residuals
residuals <- as.data.frame(rstudent(lm))
residuals <- cbind(residuals,n.index)
residuals <- residuals %>% arrange(desc(residuals))
head(residuals)

ggplot(lm,aes(x=fitted.values(lm),y=resid(lm)))+
  geom_point() +
  xlab("Fitted Value") +
  ylab("Residual Value")

ggplot(lm,aes(x=remainingWAR,y=resid(lm)))+
  geom_point() +
  xlab("WAR Remaining Before 31") +
  ylab("Residual")

```

We also need to explore influential points and outliers

```{r}
#Plot graph of influential points & outliers
hat.val <- (2*(length(train)-1))/nrow(train) #Calculate the hat value for influential points
ggplot(lm,aes(x=hatvalues(lm),y=rstudent(lm)))+
  geom_point() +
  geom_hline(yintercept=3) +       #If over 3 studentized residual, this could indicate an outlier
  geom_vline(xintercept=hat.val) + #If over hat-value, likely influential point
  labs(x="Hat Values",y="Residuals")

hatvals <- as.data.frame(hatvalues(lm))
hatvals <- cbind(hatvals,n.index)
hatvals <- hatvals %>% arrange(desc(hatvals))
head(hatvals)
```

Non-linear predictive modeling needs to be done since linear regression assumptions are violated

```{r LASSO Regression}
lasso <- glmnet(x = train_x, y = train_y, alpha = 1) #Using glmnet, LASSO has alpha of 1 (ridge is 0 and a decimal is elastic net)
plot(lasso, xvar = "lambda")

lasso_cv <- cv.glmnet(x = train_x, y = train_y, alpha = 1) #Cross validation lasso
plot(lasso_cv)

##Add in the lines for 1 standard error away from first point
plot(lasso, xvar = "lambda")
abline(v = log(lasso_cv$lambda.1se), col = "red", lty = "dashed")
abline(v = log(lasso_cv$lambda.min), col = "black", lty = "dashed")

#View the plot of the influential variables at 1 standard error from minimum mse
data.frame(summary(coef(lasso, s = lasso_cv$lambda.1se))) %>% 
  left_join((data.frame(dimnames(coef(lasso, s = lasso_cv$lambda.1se))[[1]]) %>%
             mutate(row_num = row_number()) %>%
               rename(row_name="dimnames.coef.lasso..s...lasso_cv.lambda.1se....1..")),
            by=c("i"="row_num")) %>%
  filter(row_name != "(Intercept)") %>%
  ggplot(aes(x, reorder(row_name, x))) + #Plot the influential variables
  geom_point() +
  ggtitle("Influential Variables") +
  xlab("Coefficient") +
  ylab(NULL)


```


```{r Random Forest Modeling}
set.seed(17)
ntree=250
rF <- randomForest(remainingWAR~.,data=train,importance=T,ntree=ntree)
rF
plot(rF)
set.seed(17)
t <- tuneRF(train_x, train_y,stepFactor = 0.5,plot = T,ntreeTry = 500,trace = T,improve = 0.05)
hist(treesize(rF),main = "No. of Nodes for the Trees",col = "green")
varImpPlot(rF,sort = T,n.var = 20,main = "Top 20 - Variable Importance")
importance(rF)
```


```{r Initial XGBoost Modeling}
#10 fold cross validation model testing the root mean squared error for 100 different rounds
#Will find out which number of rounds is the best based on the testing
set.seed(17)
xgModel <- xgb.cv(data = train_x, label = train_y, nrounds = 100,
                    objective = "reg:squarederror", nfold=10)

#which number of rounds has the lowest test rmse
nrounds <- which(xgModel$evaluation_log$test_rmse_mean==min(xgModel$evaluation_log$test_rmse_mean))

#tuning through caret
tune_grid <- expand.grid(
  nrounds = nrounds,
  eta = c(0.1, 0.15, 0.2, 0.25, 0.3),
  max_depth = c(1:10),
  gamma = c(0),
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = c(0.25, 0.5, 0.75, 1)
)

#Only do this the first time before the best model is found (computationally intensive)
#Find the model that is tuned with the best parameters that were chosen above to test (1 to 10 different depth levels, 5 potential eta values and 4 potential subsample values)
#set.seed(17)
#xgb.caret <- train(x = train_x, y = train_y,
#                        method = "xgbTree",
#                        tuneGrid = tune_grid,
#                        trControl = trainControl(method = 'cv', # Using 10-fold cross-validation
#                                                 number = 10))
#Look for the lowest point on this graph
#plot(xgb.caret)
#The row with the lowest mse has eta at 0.2, max_depth at 5,and subsample at 1.

set.seed(17)
best.xgb <- xgboost(data=train_x,label=train_y,eta=0.2,nrounds = nrounds,max_depth=5,subsample=1)

#Which variables were important in our best xgboost model
set.seed(17)
xg_important_vars <- xgb.importance(feature_names = colnames(train_x), model = best.xgb)
xgb.ggplot.importance(xgb.importance(feature_names = colnames(train_x), model = best.xgb))

```

Lets add a random variable to further test the significance of the importance of the variables

```{r XGboost Modeling with Random Variable Added}
train_rand <- train
train_rand$random <- rnorm(nrow(train_rand))
train_x_rand <- model.matrix(remainingWAR ~ ., data = train_rand)[, -1] 
rand.xgb <- xgboost(data=train_x_rand,label=train_y,eta=0.2,nrounds = nrounds,max_depth=5,subsample=1)
xg_important_vars_rand <- xgb.importance(feature_names = colnames(train_x_rand), model = rand.xgb)
xgb.ggplot.importance(xgb.importance(feature_names = colnames(train_x_rand), model = rand.xgb))

#Remove all variables that are not
xg_important_vars_above_rand <- xg_important_vars_rand[1:(which(xg_important_vars_rand$Feature=="random")-1)]
train_xg_simple <- train %>% dplyr::select(remainingWAR,age,RBI,R,WAR,HR,PA_per_GS,best_ranking,SLG,SO_per_PA,OBP,weight,birthDay,RBI_per_PA,height,BB,years_before_debut_best_rank,BB_per_PA,HR_per_PA,previous_accurred_WAR)
train_x_rand <- model.matrix(remainingWAR ~ ., data = train_xg_simple)[, -1] 

#10 fold cross validation model testing the root mean squared error for 100 different rounds
#Will find out which number of rounds is the best based on the testing
set.seed(17)
xgModel_rand <- xgb.cv(data = train_x_rand, label = train_y, nrounds = 100,
                  objective = "reg:squarederror", nfold=10)

#which number of rounds has the lowest test rmse
nrounds_rand <- which(xgModel_rand$evaluation_log$test_rmse_mean==min(xgModel_rand$evaluation_log$test_rmse_mean))


#Only do this the first time before the best model is found (computationally intensive)
#Find the model that is tuned with the best parameters that were chosen above to test (1 to 10 different depth levels, 5 potential eta values and 4 potential subsample values)

#tuning through caret
tune_grid_rand <- expand.grid(
  nrounds = nrounds_rand,
  eta = c(0.1, 0.15, 0.2, 0.25, 0.3),
  max_depth = c(1:10),
  gamma = c(0),
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = c(0.25, 0.5, 0.75, 1)
)
set.seed(17)
#xgb.caret_rand <- train(x = train_x_rand, y = train_y,
#                        method = "xgbTree",
#                        tuneGrid = tune_grid_rand,
#                        trControl = trainControl(method = 'cv', # Using 10-fold cross-validation
#                                                 number = 10))
#Look for the lowest point on this graph
#plot(xgb.caret)
#The row with the lowest mse has eta at 0.25, max_depth at 5, and subsample at 1.

set.seed(17)
best.xgb2 <- xgboost(data=train_x_rand,label=train_y,eta=0.25,nrounds = nrounds_rand,max_depth=5,subsample=1)

#Which variables were important in our best xgboost model
set.seed(17)
xgb.importance(feature_names = colnames(train_x_rand), model = best.xgb2)
xgb.ggplot.importance(xgb.importance(feature_names = colnames(train_x_rand), model = best.xgb2))

```

Compare results with the validations data set

```{r}
#Bring back in the id to find out which observations are which in determining worst cases
train <- cbind(train_id,train)
validation <- cbind(validation_id,validation)

#Used for all R-squared calculations as the Sum of Deviance Squared
avg <- mean(validation_y)
dev <- validation_y - avg
dev_sq <- sum(dev^2)

```

```{r}
###LASSO model validation------
validation$pred_lasso <- predict(lasso, s = lasso_cv$lambda.1se, newx = validation_x)
validation$lasso_diff <- validation$remainingWAR - validation$pred_lasso
validation_lasso <- validation %>% arrange(lasso_diff)


#Lasso specific predictions and difference
predictions_lasso <- predict(lasso, s = lasso_cv$lambda.1se, newx = validation_x)
diff_lasso <- validation_y - predictions_lasso
diff_sq_lasso <- sum(diff_lasso^2)

rsq_lasso <- 1-(diff_sq_lasso/dev_sq)
rsq_lasso #0.5105183
#The R-squared for the training was 0.4520 and is 0.5105 for the validation set


MAE_lasso <- mean(abs(validation$pred_lasso))
MAE_lasso #10.63867

worst_lasso <- validation_lasso %>% arrange(lasso_diff) %>% dplyr::select(c(ID,remainingWAR,pred_lasso,lasso_diff,age,WAR,previous_accurred_WAR))
worst_lasso <- head(worst_lasso,20)
worst_lasso
```

```{r}
##Random Forest Model Validation------
validation$rF_prediction <- predict(rF, newdata=validation)
validation$rF_diff <- validation$remainingWAR - validation$rF_prediction
validation_rF <- validation %>% arrange(rF_diff)

rF_predictions <- predict(rF,newdata=validation)
rF_diff <- validation_y - rF_predictions
rF_diff_sq <- sum(rF_diff^2)

rF_rsq <- 1-(rF_diff_sq/dev_sq)
rF_rsq #0.5716131
#The R-squared for the training was 0.5513 and is 0.5716 for the validation set


MAE_rF <- mean(abs(validation$rF_prediction))
MAE_rF #10.67763

worst_rF <- validation_rF %>% arrange(rF_diff) %>% dplyr::select(c(ID,remainingWAR,rF_prediction,rF_diff,age,WAR,previous_accurred_WAR))
worst_rF <- head(worst_rF,20)
worst_rF
```

```{r}
###XGBoost Model Validation----
validation$pred_xg <- predict(best.xgb, validation_x)
validation$xg_diff <-  validation$remainingWAR - validation$pred_xg
validation_xg <- validation %>% arrange(xg_diff)

xg_predictions <- predict(best.xgb,validation_x)
xg_diff <- validation_y - xg_predictions
xg_diff_sq <- sum(xg_diff^2)

xg_rsq <- 1-(xg_diff_sq/dev_sq)
xg_rsq #0.6321746

MAE_xg <- mean(abs(validation$pred_xg))
MAE_xg #10.39506

worst_xg <- validation_xg %>% arrange(xg_diff)%>% dplyr::select(c(ID,remainingWAR,pred_xg,xg_diff,age,WAR,previous_accurred_WAR))
worst_xg <- head(worst_xg,20)
worst_xg


validation_xg_simple <- validation %>% dplyr::select(remainingWAR,age,RBI,R,WAR,HR,PA_per_GS,best_ranking,SLG,SO_per_PA,OBP,weight,birthDay,RBI_per_PA,height,BB,years_before_debut_best_rank,BB_per_PA,HR_per_PA,previous_accurred_WAR)
validation_x_simple <- model.matrix(remainingWAR ~ ., data = validation_xg_simple)[, -1]

validation$pred_xg2 <- predict(best.xgb2, validation_x_simple)
validation$xg_diff2 <-  validation$remainingWAR - validation$pred_xg2
validation_xg2 <- validation %>% arrange(xg_diff2)

xg_predictions2 <- predict(best.xgb2,validation_x_simple)
xg_diff2 <- validation_y - xg_predictions2
xg_diff_sq2 <- sum(xg_diff2^2)

xg_rsq2 <- 1-(xg_diff_sq2/dev_sq)
xg_rsq2 #0.5952848
#If we use the second XGBoost model with less variables, we can get a similar R-squared albeit lower.
#These are all variables that were worse than our randomly inputted variable and is simpler

MAE_xg2 <- mean(abs(validation$pred_xg2))
MAE_xg2 #10.41589

worst_xg2 <- validation_xg2 %>% arrange(xg_diff2)%>% dplyr::select(c(ID,remainingWAR,pred_xg2,xg_diff2,age,WAR,previous_accurred_WAR))
worst_xg2 <- head(worst_xg2,20)
worst_xg2

```

```{r}
#Ensemble the models together
validation$pred_ensemble <- 0
for(i in 1:nrow(validation)){
  validation$pred_ensemble[i] <- (validation$pred_xg[i]+validation$rF_prediction[i]+validation$pred_lasso[i])/3
}
validation$ensemble_err <- validation$remainingWAR - validation$pred_ensemble
ensemble_err_sq <- sum(validation$ensemble_err^2)
ensemble_rsq <- 1-(ensemble_err_sq/dev_sq)
ensemble_rsq 

MAE_ensemble <- mean(abs(validation$pred_ensemble))
MAE_ensemble

validation_ensemble <- validation %>% arrange(ensemble_err)
worst_ensemble <- validation_ensemble %>% arrange(ensemble_err)%>% dplyr::select(c(ID,remainingWAR,pred_ensemble,ensemble_err,age,WAR,previous_accurred_WAR))
worst_ensemble <- head(worst_ensemble,20)
worst_ensemble

```

```{r}
#Make a table of all the results of the different modeling methods
results <- NULL
results <- rbind(results,c("LASSO",0.4848,rsq_lasso,MAE_lasso,worst_lasso$lasso_diff[1]))
colnames(results) <- c("Modeling Method","Training R-Squared","Validation R-Squared","MAE","Worst Over Prediction Difference")
results <- rbind(results,c("Random Forest Model",rF$rsq[250],rF_rsq,MAE_rF,worst_rF$rF_diff[1]))
results <- rbind(results,c("XG Boost Model",0.5849,xg_rsq,MAE_xg,worst_xg$xg_diff[1]))
results <- rbind(results,c("Simplified XG Boost Model",0.5621,xg_rsq2,MAE_xg2,worst_xg2$xg_diff2[1]))
results <- rbind(results,c("Ensemble of All Models",NA,ensemble_rsq,MAE_ensemble,worst_ensemble$ensemble_err[1]))
results <- as.data.frame(results)
results$`R-Squared Difference` <- as.numeric(results$`Validation R-Squared`) - as.numeric(results$`Training R-Squared`)
results

```
```{r XGBoost Shapley Values}
#Shapely values from XGBoost Package

xgb.plot.shap(data = as.matrix(train_x),model = best.xgb)
xgb.plot.shap.summary(data = as.matrix(train_x),model = best.xgb)
xgb.ggplot.shap.summary(data = as.matrix(train_x),model = best.xgb) +
  ylab("Shapley Value") +
  xlab("Top 10 Features") +
  labs(color="Feature Value") +
  scale_y_discrete(labels=c('OBP'='OBP',
                            'RBI'='RBI',
                            'R'='R',
                            'WAR'='WAR',
                            'HR'='HR',
                            'SLG'='SLG',
                            'age'="Age",
                            'PA_per_GS'="PA/GS",
                            'best_rankingNever Ranked'="Player was Never Ranked",
                            'SO_per_PA'="SO/PA"))


```

```{r}
# Select the observation index for which you want to compute SHAP values
point <- 1

# Create the iml Predictor object
predictor <- Predictor$new(best.xgb, data = as.data.frame(train_x), y = train_y,
                           predict.fun=function(model, newdata){
                             newData_x = xgb.DMatrix(data.matrix(newdata), missing = NA)
                             results<-predict(model, newData_x)
                             return(results)
                           })

# Create a new data frame with the same feature names as the training data
train_x
data.frame(train_x[1,])
as.matrix(newdata)
newdata <- t(data.frame(train_x[point, ]))
rownames(newdata) <- NULL
newdata <- data.frame(newdata)
colnames(newdata) <- colnames(train_x)

# Create the Shapley object and compute SHAP values for the selected observation
shapley <- Shapley$new(predictor, x.interest = data.frame(xgb.DMatrix(data.matrix(newdata))))

# Plot the SHAP values for the selected observation
shapley$plot()

```



```{r Support Vector Machine}
svmfit <- svm(remainingWAR~., data=train, kernel="linear",  cost=10, scale =FALSE )
plot(svmfit, train)  # gives SVM classification plot
svmfit$index    # gives ix of support vectors
summary(svmfit)
tune.out <-tune(svm, remainingWAR~., data=train, kernel ="linear",
              ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100) ))
summary(tune.out)
##--- Extract the model with best cost
bestmod <- tune.out$best.model
summary(bestmod)
tab <- table(bestmod$fitted , train$remainingWAR)
dat.te <- data.frame(x=Khan$xtest , y=as.factor (Khan$ytest ))
pred.te <- predict (out , newdata =dat.te)
table(pred.te , dat.te$y)
svmfit$index    
## support vectors are different now
```

```{stan Bayesian Model}
###BAYESIAN MODELING METHODS###-----------------------------
#file <- rstan::stan('Orioles One Var Bayesian.stan')

# Run the function on validation_x
#y_pred_r <- gen_quant_r(validation_x)
#mean(y_pred_r == validation_y)



```

```{r}
library(rstanarm)
library(bayesplot)

stan_model <-stan_glm(remainingWAR~.,data=train, seed=828,refresh=0)
summary(model1)

## compare to frequentist!!

model2<-lm(Sale_Price~.,data=train_reg)
summary(model2)


sims <- as.array(model1)
plot_title <- ggtitle("Posterior distributions",
                      "with medians and 95% intervals")
mcmc_areas(sims, prob = 0.95) + plot_title



sims2<-sims[,,-c(1,3,7,8)]
plot_title <- ggtitle("Posterior distributions",
                      "with medians and 95% intervals")
mcmc_areas(sims2, prob = 0.95) + plot_title



mcmc_areas(sims,
           pars = c("Lot_Area"),
           prob = 0.95) + plot_title

quantile(sims[,,3], probs = c(.025,.975))

color_scheme_set("mix-blue-red")
mcmc_trace(sims, pars = c("Age", "sigma"), 
           facet_args = list(ncol = 1, strip.position = "left"))

```

```{r Shapley Values}
# To return the SHAP values and ranked features by mean|SHAP|
var_importance_train <- NULL
for(i in 1:nrow(train_x)){
  shap_values <- shap.values(xgb_model = best.xgb, X_train = xgb.DMatrix(t(data.matrix(train_x[i,])), missing = NA))
  var_importance_train <- rbind(var_importance_train,t(data.frame(shap_values$mean_shap_score)))
  row.names(var_importance_train) <- NULL
}
var_importance_train <- data.frame(train_id,var_importance_train)

# To return the SHAP values and ranked features by mean|SHAP|
var_importance_valid <- NULL
for(i in 1:nrow(validation_x)){
  shap_values <- shap.values(xgb_model = best.xgb, X_train = xgb.DMatrix(t(data.matrix(validation_x[i,])), missing = NA))
  var_importance_valid <- rbind(var_importance_valid,t(data.frame(shap_values$mean_shap_score)))
  row.names(var_importance_valid) <- NULL
}
var_importance_valid <- data.frame(validation_id,var_importance_valid)

```

```{r}
worst_cases_var_importance <- var_importance_valid %>% filter(ID %in% worst_xg$ID | ID %in% worst_ensemble$ID)
worst_cases_var_importance
worst_cases_average_var_importance <- data.frame(t(colMeans(worst_cases_var_importance[,2:length(worst_cases_var_importance)])))
row.names(worst_cases_average_var_importance) <- NULL
worst_cases_average_var_importance
```

```{r}
# Normalize the data (optional, but recommended for neural networks)
train_x_scaled <- scale(train_x)
validation_x_scaled <- scale(validation_x)
test_x_scaled <- scale(test_x)


model <- keras_model_sequential()

model %>%
  layer_dense(units = 64, activation = 'relu', input_shape = ncol(train_x_scaled)) %>%
  layer_dense(units = 32, activation = 'relu') %>%
  layer_dense(units = 1)  # Output layer for regression

model %>% compile(
  loss = 'mean_squared_error',
  optimizer = optimizer_adam()  # You can choose other optimizers as well
)

# Set up early stopping
early_stopping <- callback_early_stopping(
  monitor = 'val_loss',
  patience = 10,
  restore_best_weights = TRUE
)

# Train the model
history <- model %>% fit(
  x_train, y_train,
  epochs = 100,
  batch_size = 32,
  validation_data = list(validation_x_scaled, validation_y),
  callbacks = list(early_stopping)
)


```